{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store pdf into vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load pdf\n",
    "loader = PyPDFLoader(\"ally.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split pdf images into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
    "texts = splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Embed pdfs\n",
    "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "vectorstore = FAISS.from_documents(texts, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"vector_store_ally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# embedding engine\n",
    "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# load from local\n",
    "db = FAISS.load_local(\"vector_store_ally\", embeddings=embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import (\n",
    "    ConversationalRetrievalChain, \n",
    "    ConversationChain, \n",
    "    RetrievalQAWithSourcesChain, \n",
    "    RetrievalQA\n",
    ")\n",
    "\n",
    "from langchain.chains.conversation.memory import (\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationBufferMemory, \n",
    "    ConversationSummaryBufferMemory\n",
    ")\n",
    "\n",
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(\n",
    "      template = prompt_template, \n",
    "      input_variables = [\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "      search_kwargs = {'k': 2}, \n",
    "      chain_type_kwargs = chain_type_kwargs,\n",
    "      return_source_documents = True\n",
    ")\n",
    "\n",
    "llm = CTransformers(\n",
    "      model = \"models/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "      model_type = \"llama\",\n",
    "      config = {\n",
    "            'max_new_tokens':512,\n",
    "            'temperature':0.8}\n",
    "      )\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "      llm = llm, \n",
    "      chain_type = \"stuff\", \n",
    "      retriever = retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What was the provision for credit losses decrease at the end of 2021?'\n",
    "ans = qa({'query':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print query, result, and source_documents\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results only\n",
    "print(ans['result'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9fb0e67e485f39ba6ef3b8dc2a4a9e0f768372cd283c217e1233d090d1c84713"
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
